{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "# List of modules to check and install if needed\n",
    "required_packages = [\n",
    "    \"netCDF4\", \"rasterio\", \"numpy\",  \"xarray\", \"dask\", \"rioxarray\",\n",
    "    \"matplotlib\", \"cartopy\", \"geopandas\", \"pandas\", \"pyproj\",  \"shapely\", \"fiona\"\n",
    "]\n",
    "\n",
    "def install_and_import(package):\n",
    "    try:\n",
    "        importlib.import_module(package)\n",
    "    except ImportError:\n",
    "        print(f\"Installing {package}...\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "    finally:\n",
    "        globals()[package] = importlib.import_module(package)\n",
    "\n",
    "for pkg in required_packages:\n",
    "    install_and_import(pkg)\n",
    "\n",
    "import xarray as xr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New working directory set for Step 4: /Users/kluis/PycharmProjects/MERIS_TSM\n",
      "Cleanup process complete.\n"
     ]
    }
   ],
   "source": [
    "# 1 Unzip and extract all .ZIP files in base directory & then delete original .ZIP files\n",
    "\n",
    "# Set the base directory\n",
    "#base_directory = \"/Users/lopezama/Documents/Blackwood/MERIS/scripts/earth_data/test_data\"\n",
    "base_directory = \"/Users/kluis/PycharmProjects/MERIS_TSM/\"\n",
    "\n",
    "# Define function\n",
    "def unzip_and_delete(directory):\n",
    "    \"\"\"Unzips all zip files in a directory and then deletes the original zip files.\"\"\"\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(\".ZIP\"):\n",
    "            file_path = os.path.join(directory, filename)\n",
    "            try:\n",
    "                with zipfile.ZipFile(file_path, 'r') as zip_ref:\n",
    "                    zip_ref.extractall(directory)\n",
    "                os.remove(file_path)\n",
    "                print(f\"Unzipped and deleted: {filename}\")\n",
    "            except zipfile.BadZipFile:\n",
    "                print(f\"Skipping invalid zip file: {filename}\")\n",
    "            except Exception as e:\n",
    "                print(f\"An error occurred processing {filename}: {e}\")\n",
    "\n",
    "# Execute step\n",
    "unzip_and_delete(base_directory)\n",
    "\n",
    "# Loop through all items in the base directory\n",
    "for item in os.listdir(base_directory):\n",
    "    item_path = os.path.join(base_directory, item)\n",
    "\n",
    "    # Process only directories that end with \".SEN3\" (case-insensitive)\n",
    "    if os.path.isdir(item_path) and item.upper().endswith('.SEN3'):\n",
    "        new_folder_name = item[16:31] + \"_use\" # Designate which filename characters to use for rename\n",
    "        new_folder_path = os.path.join(base_directory, new_folder_name)\n",
    "        \n",
    "        # Check to ensure the new folder name doesn't already exist\n",
    "        if not os.path.exists(new_folder_path):\n",
    "            os.rename(item_path, new_folder_path)\n",
    "            print(f\"Renamed folder: {item} -> {new_folder_name}\")\n",
    "\n",
    "            # Update base_directory to the newly renamed folder\n",
    "            base_directory = new_folder_path  # **Update base_directory globally**\n",
    "            print(f\"New working directory: {base_directory}\")\n",
    "\n",
    "files_to_keep = [\n",
    "    \"cloud.nc\", \"common_flags.nc\", \"cqsf.nc\", \"geo_coordinates.nc\",\n",
    "    \"iop_nn.nc\", \"par.nc\", \"tie_geo_coordinates.nc\", \"tie_geometries.nc\",\n",
    "    \"time_coordinates.nc\", \"tsm_nn.nc\", \"wqsf.nc\"\n",
    "]\n",
    "\n",
    "# **Only process the updated base_directory (not all folders in parent directory)**\n",
    "if os.path.isdir(base_directory) and base_directory.endswith(\"_use\"):\n",
    "    print(f\"Processing folder for cleanup: {base_directory}\")\n",
    "\n",
    "    for file in os.listdir(base_directory):\n",
    "        file_path = os.path.join(base_directory, file)\n",
    "\n",
    "        if os.path.isfile(file_path) and file not in files_to_keep:\n",
    "            os.remove(file_path)\n",
    "            print(f\"Deleted: {file_path}\")\n",
    "        else:\n",
    "            print(f\"Kept: {file_path}\")\n",
    "\n",
    "\n",
    "# Explicitly change to the new working directory before Step 4\n",
    "os.chdir(base_directory)\n",
    "print(f\"New working directory set for Step 4: {os.getcwd()}\")\n",
    "\n",
    "print(\"Cleanup process complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "One or both NetCDF files are missing in the directory.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m geocoord_nc \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(base_directory, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgeo_coordinates.nc\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(tsm_nc) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(geocoord_nc):\n\u001b[0;32m---> 11\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOne or both NetCDF files are missing in the directory.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     13\u001b[0m ds_tsm \u001b[38;5;241m=\u001b[39m xr\u001b[38;5;241m.\u001b[39mopen_dataset(tsm_nc)\n\u001b[1;32m     14\u001b[0m ds_geocoord \u001b[38;5;241m=\u001b[39m xr\u001b[38;5;241m.\u001b[39mopen_dataset(geocoord_nc)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: One or both NetCDF files are missing in the directory."
     ]
    }
   ],
   "source": [
    "# Add lat/lon as spatial dimensions to TSM netcdf from geo_coordinates netcdf\n",
    "\n",
    "# A) Open the NetCDF files\n",
    "\n",
    "# NetCDF with data but missing lat/lon dimensions\n",
    "tsm_nc = os.path.join(base_directory, \"tsm_nn.nc\")\n",
    "# NetCDF containing lat/lon dimensions\n",
    "geocoord_nc = os.path.join(base_directory,,  \"geo_coordinates.nc\")\n",
    "\n",
    "if not os.path.exists(tsm_nc) or not os.path.exists(geocoord_nc):\n",
    "    raise FileNotFoundError(\"One or both NetCDF files are missing in the directory.\")\n",
    "\n",
    "ds_tsm = xr.open_dataset(tsm_nc)\n",
    "ds_geocoord = xr.open_dataset(geocoord_nc)\n",
    "\n",
    "# B) Extract latitude & longitude\n",
    "lat = ds_geocoord[\"latitude\"].values  # Extract lat as NumPy array\n",
    "lon = ds_geocoord[\"longitude\"].values  # Extract lon as NumPy array\n",
    "\n",
    "# C) Ensure lat/lon are correctly formatted\n",
    "if lat.ndim == 2:  # If 2D, extract unique values along the correct axis\n",
    "    lat = lat[:, 0]  # Take the first column (assuming lat is constant across rows)\n",
    "if lon.ndim == 2:\n",
    "    lon = lon[0, :]  # Take the first row (assuming lon is constant across columns)\n",
    "\n",
    "print(f\"Latitude shape after fix: {lat.shape}\")  # Should be (4289,)\n",
    "print(f\"Longitude shape after fix: {lon.shape}\")  # Should be (4481,)\n",
    "\n",
    "# D) Extract Data & Ensure Correct Shape\n",
    "var_name = list(ds_tsm.data_vars.keys())[0]  # Get first variable name\n",
    "data_values = ds_tsm[var_name].values  # Extract data\n",
    "\n",
    "# If data is 3D (e.g., time, lat, lon), select the first time step\n",
    "if data_values.ndim == 3:\n",
    "    data_values = data_values[0, :, :]\n",
    "\n",
    "# Ensure the data shape matches (lat, lon)\n",
    "if data_values.shape != (len(lat), len(lon)):\n",
    "    raise ValueError(\n",
    "        f\"Mismatch: data shape {data_values.shape} vs expected ({len(lat)}, {len(lon)})\"\n",
    "    )\n",
    "\n",
    "print(f\"Final Data shape: {data_values.shape}\")  # Should match (4289, 4481)\n",
    "\n",
    "# E) Create a new NetCDF dataset with correct dimensions\n",
    "ds_new = xr.Dataset(\n",
    "    {\n",
    "        var_name: ([\"latitude\", \"longitude\"], data_values)\n",
    "    },\n",
    "    coords={\n",
    "        \"latitude\": (\"latitude\", lat),  # Assign dimensions explicitly\n",
    "        \"longitude\": (\"longitude\", lon)\n",
    "    }\n",
    ")\n",
    "\n",
    "# F) Save the updated dataset\n",
    "tsm_nc_spdm = os.path.join(base_directory, \"tsm_nn_spdm.nc\")\n",
    "ds_new.to_netcdf(tsm_nc_spdm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm lat/lon were added as spatial dimensions to netCDF \n",
    "tsm_nc_spdm_view = xr.open_dataset(\"/Users/lopezama/Documents/Blackwood/MERIS/scripts/earth_data/test_data/20120407T182134_use/tsm_nn_spdm.nc\")\n",
    "tsm_nc_spdm_view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show min and max variable values in netCDF\n",
    "\n",
    "def find_min_max(file_path, variable_name):\n",
    "    \"\"\"\n",
    "    Finds the minimum and maximum values of a specified variable within a NetCDF file.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): The path to the NetCDF file.\n",
    "        variable_name (str): The name of the variable to analyze.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A tuple containing the minimum and maximum values.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with Dataset(file_path, 'r') as nc_file:\n",
    "            if variable_name not in nc_file.variables:\n",
    "                raise ValueError(f\"Variable '{variable_name}' not found in the file.\")\n",
    "            \n",
    "            variable_data = nc_file.variables[variable_name][:]\n",
    "            \n",
    "            if np.size(variable_data) == 0:\n",
    "                 raise ValueError(f\"Variable '{variable_name}' has no data.\")\n",
    "\n",
    "            min_value = np.nanmin(variable_data)\n",
    "            max_value = np.nanmax(variable_data)\n",
    "            \n",
    "            if np.isnan(min_value) or np.isnan(max_value):\n",
    "                raise ValueError(f\"Could not determine min/max values for '{variable_name}'. Ensure the data does not exclusively contain NaN values.\")\n",
    "\n",
    "            return min_value, max_value\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "    except Exception as e:\n",
    "         raise Exception(f\"An error occurred: {e}\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    file_path = '/Users/lopezama/Documents/Blackwood/MERIS/scripts/earth_data/test_data/20120407T182134_use/tsm_nn_spdm.nc'  # Replace with the actual path to your NetCDF file\n",
    "    variable_name = 'TSM_NN'  # Replace with the name of the variable you want to analyze\n",
    "\n",
    "    try:\n",
    "        min_val, max_val = find_min_max(file_path, variable_name)\n",
    "        print(f\"Minimum value of '{variable_name}': {min_val}\")\n",
    "        print(f\"Maximum value of '{variable_name}': {max_val}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show values of first 20 pixels in netCDF\n",
    "\n",
    "def print_netcdf_to_dataframe(file_path, variable_name):\n",
    "    \"\"\"\n",
    "    Prints data from a NetCDF file to a Pandas DataFrame for the first 10 pixels.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): Path to the NetCDF file.\n",
    "        variable_name (str): Name of the variable to extract.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with nc.Dataset(file_path, 'r') as nc_file:\n",
    "            variable_data = nc_file.variables[variable_name][:]\n",
    "\n",
    "            if variable_data.ndim < 2:\n",
    "                print(\"Variable must have at least 2 dimensions (e.g., latitude, longitude).\")\n",
    "                return\n",
    "\n",
    "            num_pixels = min(20, variable_data.shape[-1])\n",
    "            data_values = variable_data[..., :num_pixels]\n",
    "\n",
    "            df = pd.DataFrame(data=data_values.reshape(-1, num_pixels))\n",
    "            print(df)\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File not found: {file_path}\")\n",
    "    except KeyError:\n",
    "        print(f\"Error: Variable '{variable_name}' not found in the file.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred: {e}\")\n",
    "\n",
    "\n",
    "# Path to netcdf you want to look at\n",
    "file_path = '/Users/lopezama/Documents/Blackwood/MERIS/scripts/earth_data/test_data/20120407T182134_use/tsm_nn_spdm.nc'\n",
    "\n",
    "# Specify the variable you want to look at\n",
    "variable_name = 'TSM_NN'\n",
    "\n",
    "# Display the values\n",
    "print_netcdf_to_dataframe(file_path, variable_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clip TSM netCDF using lat/lon ROI shapefile\n",
    "\n",
    "def crop_netcdf_with_shapefile(nc_file, shapefile, output_nc, data_var=\"TSM_NN\"):\n",
    "    \"\"\"\n",
    "    Crops a NetCDF file using a shapefile and logs a message if no data is found.\n",
    "\n",
    "    :param nc_file: Path to the input NetCDF file.\n",
    "    :param shapefile: Path to the shapefile defining the crop region.\n",
    "    :param output_nc: Path to save the cropped NetCDF file.\n",
    "    :param data_var: The variable to extract from the NetCDF.\n",
    "    \"\"\"\n",
    "    # Load NetCDF\n",
    "    ds = xr.open_dataset(nc_file)\n",
    "\n",
    "    # Ensure the dataset is georeferenced\n",
    "    if \"spatial_ref\" not in ds:\n",
    "        ds = ds.rio.write_crs(\"EPSG:4326\")  # Set default CRS if missing\n",
    "\n",
    "    # Load the shapefile\n",
    "    gdf = gpd.read_file(shapefile)\n",
    "\n",
    "    # Convert to dataset CRS if different\n",
    "    if gdf.crs is not None and gdf.crs != ds.rio.crs:\n",
    "        gdf = gdf.to_crs(ds.rio.crs)\n",
    "\n",
    "    # Get geometry from the shapefile\n",
    "    geom = [mapping(geometry) for geometry in gdf.geometry]\n",
    "\n",
    "    # Define log file path\n",
    "    log_file = os.path.join(os.path.dirname(output_nc), \"log.txt\")\n",
    "\n",
    "    try:\n",
    "        # Attempt to clip the NetCDF using the shapefile geometry\n",
    "        clipped_ds = ds.rio.clip(geom, gdf.crs, drop=True)\n",
    "\n",
    "        # Check if there is data inside the clipped region\n",
    "        if data_var in clipped_ds and clipped_ds[data_var].count() == 0:\n",
    "            raise NoDataInBounds(f\"No data found in bounds. Data variable: {data_var}\")\n",
    "\n",
    "        # Save the cropped NetCDF\n",
    "        clipped_ds.to_netcdf(output_nc)\n",
    "        print(f\"✅ Cropped NetCDF saved to: {output_nc}\")\n",
    "\n",
    "    except NoDataInBounds as e:\n",
    "        # Write log file if NoDataInBounds error occurs\n",
    "        with open(log_file, \"w\") as log:\n",
    "            log.write(f\"NoDataInBounds: No data found in ROI bounds. Data variable: {data_var}.\\n\")\n",
    "        print(f\"⚠️ No data found in ROI bounds. Log saved: {log_file}\")\n",
    "\n",
    "# Example Usage\n",
    "tsm_nc_spdm = \"/Users/lopezama/Documents/Blackwood/MERIS/scripts/earth_data/test_data/20110731T182333_use/tsm_nn_spdm.nc\"\n",
    "shp_ll = \"/Users/lopezama/Documents/Blackwood/MERIS/ROI/west_us_poly_ll/west_us_poly_ll.shp\"\n",
    "tsm_nc_spdm_clip = \"/Users/lopezama/Documents/Blackwood/MERIS/scripts/earth_data/test_data/20110731T182333_use/tsm_nn_spdm_clip.nc\"\n",
    "\n",
    "crop_netcdf_with_shapefile(tsm_nc_spdm, shp_ll, tsm_nc_spdm_clip)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
